
# 윤송이 엔씨소프트 사장이 말하는 'AI 시대의 윤리'

Published at: **2019-11-04T07:08:42+00:00**

Author: **이두현 기자**

Original: [INVEN](http://www.inven.co.kr/webzine/news/?news=229431)

기술이 가진 힘은 강력합니다. 인류 발전의 결정적인 순간에는 항상 과학 기술이 있었기에 그 힘은 의심할 여지가 없습니다. 현재도 과학 기술은 하루가 다르게 발전하고 있고 이는 새로운 세상으로의 진입 속도를 더 빠르게 앞당기고 있습니다.
그럼 우리는, 기술을 받아들이는 준비 또한 그 속도에 맞춰서 잘하고 있는 걸까요. AI가 현실과 점점 더 밀접해지고 있는 지금 문제가 되는 건 정말 우리의 일자리뿐일까요. 'SCIENCE to the Future' 시리즈의 마지막은 도래하는 AI시대에 생각해봐야 할 윤리 문제를 다룹니다. 기술을 바라보는 시각을 넘어, 미래를 바라보는 시각을 달리하게 되는 계기가 될 것입니다.
윤송이 사장은 현재 미국 스탠포드대학 인간 중심 AI연구소(Human-Centered AI Institute, HAI)의 자문 위원을 맡고 있다. 인간 중심 AI연구소는 AI와 데이터가 더 광범위하게 쓰이는 사회에서 야기될 수 있는 문제들에 의식을 가진 각계의 인사들을 주축으로 운영되며, 에릭 슈미트 전 구글 회장, 제리 양 야후 공동 창업자, 제프 딘 구글 AI 책임자 등이 자문위원으로 활동하고 있다. 자문위원들은 미래 사회에서 기술 개발, 교육, 정책, 법제 마련은 어떤 식으로 되어야 하는지 의견을 낸다. 자문위원들의 의견은 일반 대중을 위한 지침서와 제안서를 마련하고 이들을 위한 교육 시행에 활용된다.
AI도 선입견이 있다자율 주행 자동차가 도입되는 세상을 이야기할 때 흔히 거론되는 문제가 있습니다. 바로 트롤리 딜레마(Trolley Problem)에 관한 것입니다. 자율 주행 자동차가 왼쪽으로 꺾으면 탑승자, 즉 차 주인이 다치게 되고 오른쪽으로 꺾으면 여러 명의 유치원생들이 다치게 되는 상황에서 핸들을 어느 쪽으로 꺾도록 프로그램이 되어야 할까 하는 문제가 그것입니다.
자율 주행 자동차가 주행 중 맞닥뜨릴 수 있는 여러 상황에서의 도덕적 판단을 프로그래밍 하기 위해서는 판단의 기준은 누가 정할 것인지, 어떤 기준으로 판단할 것인지에 대한 논의가 선행되어야 합니다. 하지만 이 논의의 배경에 얼마나 많은 가정들이 고려되었는지, 이와 관련된 상위 인지 문제를 풀기 위해서 얼마나 많은 단계를 거쳐야 하는지에 대해서는 정작 충분하게 논의되지 못하고 있습니다.
상위 인지(metacognition): 자신의 인지활동에 대한 인지를 뜻한다. 자신이 무엇을 알고 무엇을 모르는지에 대해 아는 것에서부터 모르는 부분을 보완하기 위한 계획을 세우고 이를 실행하는 전 과정을 말한다.
2018년 초 미국 매사추세츠공과대학(MIT)에서 발표한 내용에 따르면, 오픈 소스로 흔하게 쓰이고 있는 얼굴 인식 알고리즘은 피부색과 성별에 따라 인식률에 차이가 있다고 합니다. 백인 남성의 경우 98%의 정확도로 인식하는 반면, 유색 여성의 경우 70%가 채 안 되는 인식률을 보인 것입니다. 자율 주행 자동차는 이런 카메라 기반의 얼굴 인식 알고리즘 외에도 다양한 센서로 작동합니다. 하지만 이 얼굴 인식 알고리즘이 왼쪽에는 사람이 있고, 오른쪽에는 유인원이 있다는 결론을 내린다면, 앞서 언급한 상위 인지 문제에 도달하기도 전에 특정 인종에 불합리한 의사 결정을 하게 될 수 있다는 결론에 이르게 됩니다.
이런 편견과 불합리성이 내재된 소프트웨어는 어떻게 만들어졌을까요? 엔지니어들이 지독한 편견을 가지고 있어서일까요? 그렇게 생각되지는 않습니다. 그들이 AI를 학습시키는데 사용한 데이터 자체에 백인 남성의 데이터가 더 많다 보니 그렇게 학습되었을 가능성이 높습니다.
지금은 그렇지 않지만, 몇 년 전까지만 해도 구글 검색창에 CEO라는 단어를 치면 이미지 검색 결과의 상위 50개는 모두 백인 남성의 사진이었습니다. 여성과 관련된 사진 중 가장 상위에 나오는 건 사람의 사진도 아닌 미국 완구업체 마텔사에서 만든 CEO 모습을 한 바비 인형 사진이었습니다. 이런 데이터로 학습된 AI에게 ’CEO는 어떤 사람일까’라는 질문을 했을 때 어떤 대답이 나올지는 자명한 일입니다.
그런데 AI가 이런 선입견을 가지는 게 잘못된 것일까요? 누군가는 편견을 가지고 있는 AI가 불편할 수 있습니다. 또 다른 누군가는 그것이 사회의 현실이기 때문에 편향된 시각을 가지는 게 제대로 된 AI라고 생각할 수도 있습니다. 그럼 우리가 꿈꾸는 미래의 사회는 어떤 모습일까요? AI를 더 많이 만들어 보급해야 한다고 말하기 전에, 이런 질문에 대한 성숙한 사회적 논의가 먼저 이루어져야 하지 않을까요?
기술은 편견이나 불공정함을 여과 없이 담는다배심원제도는 미국 재판제도의 대표적인 특징 중 하나입니다. 미국의 경우 법정에 나오는 판사, 변호사, 검사 모두 법정에서 보내는 시간에 대한 보상과 처우를 받습니다. 그러나 일반 시민 중 무작위로 선출되는 배심원들은 무료로 참여합니다. 국민의 의무로 기꺼이 봉사하는 셈입니다.
하지만 재판은 하루 이틀 안에 끝나는 것이 아니라 사안에 따라 한 달가량 지속되기도 합니다. 일용직 노동자들에겐 여간 부담되는 일이 아닐 수 없습니다. 이런 사람들은 대부분 불참 사유서를 제출해 면제를 받게 되지만, 국민의 의견을 골고루 받겠다고 만든 시스템에 특정 경제 계층의 의견이 반영되지 못하는 문제를 초래합니다. 여러 가지로 보완해야 할 점이 있는 제도입니다.
그런데 이런 제도에서 나온 판결의 결과로 학습된 AI 시스템은 어떤 판결을 내리게 될까요? 이미 미국 법원에서는 보석 결정을 위해 AI 기반의 소프트웨어를 쓰고 있습니다. 이 시스템은 특정 인종과 소득 계층의 피고인에게 불리한 결과를 내는 경향이 있다는 문제가 지적되고 있습니다. 우리 제도가 가지고 있는 편견과 불합리함을 그대로 학습한 AI 시스템이 불편해지고 있는 것입니다. 그렇다면 어떤 판결이 합리적이고 공정한 것인지 AI에게 누가 이야기해줘야 하는 걸까요?
데이터 기반으로 약을 만드는 정밀의료(precision medicine: 환자마다 다른 유전체 정보, 환경적 요인, 생활 습관 등을 분자 수준에서 종합적으로 분석하여 최적의 치료방법을 제공하는 의료서비스)도 마찬가지입니다. 인종, 체질, 성별에 따라서 어떤 약이 더 효과적일지 차이가 있다는 건 쉽게 유추할 수 있습니다. 하지만 이런 약을 만드는데 쓰이는 데이터의 대부분은 병원을 더 자주 찾는, 또 이런 실험에 기꺼이 참가할 시간과 여유가 있는 사람들의 것이라는 사실은 쉽게 간과되곤 합니다. 이런 약을 만드는데 정부의 보건 예산이 쓰이는 건 정당한 일일까요? 이러한 편견은 이미 사회에 널리 퍼져 있습니다. 하지만 같은 이야기가 기계를 통해 나오는 순간 사람들은 왠지 더 공정하고 객관적일 것이라 믿습니다. 결국 믿음의 오류는 결과의 정당화를 초래하게 되고 맙니다.
편견이 학습된 AI의 결과가 구글 검색의 첫 페이지에 있다면기술은 쉽게 디지털화 되어 획일적으로 많은 사람들에게 퍼지게 할 수 있습니다. 편견을 그대로 전파하게 되는 위험을 더하는 것입니다. 과거에는 아이들이 궁금한 게 있으면, 어른에게 물어보거나 백과사전을 찾아보기도 하고 도서관에 가서 책을 찾아보기도 하면서 다양한 해결 방법을 통해 답을 얻을 수 있었습니다. 요즘은 다들 궁금한 내용이 있으면 구글 같은 검색 엔진에서 검색하고 첫 페이지에 나오는 내용을 읽습니다. 그리고 더 이상 궁금해하지 않습니다. 이렇게 답을 찾았다고 생각하는 것이 창의력 향상에 저해가 된다는 글을 읽은 적이 있습니다.
무한 복제가 가능한 디지털 서비스가 갖는 폐해입니다. 편견을 가진 AI가 바로 이렇게 무한 복제가 되어 모든 사람들 앞에 동시에 서게 될 경우 폐해가 더 커질 수 있기 때문에 이 문제가 더 염려됩니다.
AI는 편견의 타래를 푸는 열쇠가 될 수 있다하지만 AI가 언제나 우리 사회의 편견을 심화시키기만 하는 것은 아닙니다. AI는 인간 본성을 드러내고 이에 대한 근본적인 질문을 던지는, 그리고 대답을 요구하는 계기를 제공하기도 합니다. 아마존의 인공지능 플랫폼 알렉사가 남성의 목소리를 가졌을 때와 여성 목소리를 가졌을 때 사람들이 각각을 대하는 방법이나 쓰는 단어, 목소리가 달라진다는 연구 결과가 있습니다. 이 결과는 우리 사회의 불편한 단면을 드러내기도 합니다.
기기에 말을 함부로 하거나, 심지어 욕설을 하는 건 정당한 일일까요? 아니라면 왜 불편하게 생각해야 하는 일일까요? 흔히 로봇은 인권이 없는 존재로 생각합니다. 그러면 사이보그는 어떤가요? 팔다리를 인공 팔다리로 교체한 사람의 인권이 그렇지 않은 사람의 인권보다 덜하다고 생각하는 사람은 아무도 없을 것입니다.
이렇게 사고를 하나씩 확장해 나가다 보면, 인간의 존엄은 물리적인 육체가 아니라 생각과 사고에서 나온다는 결론에 도달할 수 있습니다. 때문에 개개인의 생김새나 신체적 조건 때문에 차별을 한다는 건 너무나 부당한 것임을 알 수 있습니다. 이렇게 AI의 기술로 발견된 편견과 부당함은 오히려 편견이 어디서 오게 되었는지 풀 수 있는 실마리가 될 수 있습니다.
인간과 AI의 조화로운 공존최근 제기되는 다양한 편견의 문제들은 이미 우리 사회 곳곳에 내재되어 있었습니다. 어떻게 실타래를 풀어가야 할지 생각할 겨를이 없어 미뤄 놓았던 문제들인 것도 많습니다. 하지만 AI와 디지털 기술의 도입이 미루어 놓았던 문제들의 신속한 해결을 촉구하고 있습니다. 이미 해외 유수 대학들에서는 컴퓨터 사이언스 과목에 윤리 모듈을 접목시키는 것을 의무화하는 제도의 도입을 검토하고 있습니다.
기술이 가지는 파급력이 커지는 만큼, 이를 다루고 만드는데 따르는 책임도 커지고 있습니다. 우리가 만들어 내는 기술이 사회에 어떤 영향을 미치는지, 의도하지 않았던 결과를 초래할 위험은 없는지 충분한 고민이 필요한 시점입니다. 뿐만 아니라 편견이 반영된 기술을 받아들이지 않는 사회의 의식 또한 성숙해져야 할 것입니다.
파급력있는 기술을 만드는 입장에서 어떤 기준으로 기술을 만들고 발전시켜 나갈 것인지 고민하는 건 당연합니다. 그리고 그 기준을 만드는 데 참여하는 것은 의무입니다. 인공지능은 더 이상 하나의 새로운 기술에 그치지 않습니다. 이 기술이 사회에 올바르게 작동하기 위해선 교육, 정책, 법률 등 다양한 부문에서 이 문제를 함께 고민해야 할 것입니다.
